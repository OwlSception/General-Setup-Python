{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features\n",
    "\n",
    "### Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One-hot encoding\n",
    "- Frequency encoding\n",
    "- Target encoding\n",
    "- Binary encoding\n",
    "- Hashing\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Feature Importance \n",
    "\n",
    "\n",
    "## Scaling/Transforming Features\n",
    "\n",
    "\n",
    "## Weighting Features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "    - Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms the original features into a new set of features, while preserving as much variance as possible.\n",
    "    - Steps:\n",
    "    1. Standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "    2. Calculate the covariance matrix of the features.\n",
    "    3. Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "    4. Sort the eigenvalues in descending order.\n",
    "    5. Select the top k eigenvectors corresponding to the largest eigenvalues.\n",
    "    6. Transform the original features using the selected eigenvectors.\n",
    "    - Advantages:\n",
    "    1. Reduces the dimensionality of the data while preserving the most important information.\n",
    "    2. Can help improve the performance of machine learning algorithms.\n",
    "    - Disadvantages:\n",
    "    1. Can be computationally expensive for large datasets.\n",
    "    2. Can lead to loss of information if not carefully handled.\n",
    "\n",
    "- t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    - t-SNE is a non-linear dimensionality reduction technique that preserves the local structure and relationships between the data points in the high-dimensional space.\n",
    "    - Steps:\n",
    "    1. Standardize the features to have a mean of 0 and a standard deviation of 1.\n",
    "    2. Compute the pairwise distances between the data points.\n",
    "    3. Compute the similarity matrix using the Euclidean distance or other similarity metrics.\n",
    "    4. Perform the t-SNE algorithm to obtain a low-dimensional embedding.\n",
    "    5. Visualize the embedded data points in the low-dimensional space.\n",
    "    - Advantages:\n",
    "    1. Can help visualize the data in high-dimensional spaces.\n",
    "    2. Preserves the local structure and relationships between the data points.\n",
    "    - Disadvantages:\n",
    "    1. Slower than PCA for large datasets.\n",
    "    2. Can be sensitive to the choice of parameters (e.g., perplexity) and the random initialization.\n",
    "\n",
    "- LDA \n",
    "\n",
    "- Regularization\n",
    "\n",
    "- Hyperparameter tuning\n",
    "\n",
    "- Filtering \n",
    "\n",
    "- Wrapper Methods\n",
    "\n",
    "\n",
    "\n",
    "## Feature Interaction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
